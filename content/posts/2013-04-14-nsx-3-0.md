---
title: "VMware NSX 3.0 - lab installation"
created_at: 2013-04-14 17:48:00 +0100
kind: article
published: false
tags: ['howto', 'vmware']
---

In this article we will review all the steps required to deploy an OpenStack Nicira NVP PoC. We will embed this lab within an OpenStack cloud. In our environment each hypervisor VM is connected to both the management and data network using two distinct vNic network interfaces.

<!-- more -->

### OpenStack Networking

* Connect to your OpenStack Horizon dashboard and create the following networks
* be sure to leave Gateway unconfigured and leave DHCP unchecked.

|Network name|Subnet|
|CorpNAT|192.168.1.0/24|
|Management|10.10.10.0/24|
|transport-1|10.10.1.0/24|
|transport-2|10.10.2.0/24|

* Create a logical router to connect CorpNAT to the external world
* Create a logical router to connect both data networks together

### NVP Controllers

The NVP Controllers Cluster is a cluster of x86 servers offering: massive scalability and full fault-tolerance. They manage the edge switching components within an NVP domain.

* Within your OpenStack Cloud, launch three instances of *NVP-Controller-3.x*.
* Connect each of them to CorpNAT and Management Network
* Associate a floating IP to each instance
* Run the following commands on each of them, default login/password is admin/admin:

	ssh admin@controller-floatingIP
 	set hostname nsx-controller-<Instance#>
 	add network dns-server 8.8.8.8
 	add network ntp-server 0.pool.ntp.org
 	set network interface breth1 ip config static <mgt-network>.x 255.255.255.0
 	set control-cluster management-address <controller-mgtIPaddress>
 	set control-cluster role api_provider listen-ip <controllerIP-mgtIPaddress>
 	set control-cluster role switch_manager listen-ip <controllerIP-mgtIPaddress>
 	join control-cluster <controllerIP-mgtIPaddress>
 	show control-cluster startup-nodes
 	show control-cluster status

If when joining you get the folllowing error :

	Could not join node 192.168.1.3's cluster, because a different node with this node's management address is already part of the cluster

You need to delete the old UUID from the Controller Cluster using the DELETE  /ws.v1/control-cluster/node/<old_uuid> API method.

### NVP Manager

It's a web based interface that provides user-friendly method to interact with the NVP API. But it's not meant for day to day provisioning or automated configuration.

* Within your OpenStack Cloud, launch one instance of *NVP-Manager-3.x*.
* Connect it to both CorpNAT and Management networks
* Associate a floating IP to your instance
* Run the following commands, default login/password is admin/admin:

	ssh admin@manager-floatingIP
	set hostname nsx-manager
	add network dns-server 8.8.8.8 [seems not needed in 3.2 if DHCP configured]
	add network ntp-server 0.pool.ntp.org
	add control-cluster
	create transport zone

* Connect to the Manager WebUI at http://manager-floatingIP with admin/admin credentials
* Click `Add Cluster`, Host = <controllerIP>, Username = admin, Password = admin
* Create a `Transport Zone`, it's a collection of OVS devices that can communicate with each other.

### Service Node

Service Node offloads task of packet replication from hypervisor CPU for L2 broadcast, Multicast and Unknown unicast.

To use an alternative to provision a host, we will use PXE boot to provision our Service Node.

You first have to make sure your default security group allows UDP/69 traffic.

Before launching your instance, connect it 
to Management and Transport-1 Networks.

You can now launch a PXE instance and at the boot menu type what kind of instance you want to build based on the following pattern : 

	<node-type>-<releasecodename>-<buildnumber>

Where <node-type> could be controller, manager, service-node, gateway and <buildnumber> could be latest to get the latest working build. If you want the latest release you can simply type servicenode-latest

After a while you'll get your shiny new instance ready to rock.

* Associate a floating IP to it
* Run the following commands, default login/password is admin/admin:

	ssh admin@servicenode-floatingIP
 	set hostname nsx-servicenode
 	add network dns-server 8.8.8.8
 	add network ntp-server 0.pool.ntp.org
 	set network interface breth1 ip config static <network-data1>.5 255.255.255.0
 	add network route 10.10.0.0 255.255.0.0 <network-data1>.1
 	show network routes
 	add switch manager <controllerIP>

* Connect to the Manager WebUI at http://manager-floatingIP with admin/admin credentials to create Service Node API Entity
* Click `Add` in the Service Node Row in Summary of Transport Components section
* Leave all values default exept
	Transport Node Type: Service Node
	Display Name: nsx-servicenode-1
* Get SSL certification from Service Node using:
	show switch certificate
* Paste SSL certificate in Security Certificate textbox
* Click `Add Connector`, to add a Transport Connector
	Transport Type: STT
	Transport Zone UUID: only one available here
	IP Address: <network-data1>.5
* Click `Save & View`, to register Service Node
* Check Service Node Status from Manager GUI, it should display Management Connection up,  OpenFlow Connection up and admin status enabled.
* Check Service Node Status from SSH
	show switch managers

Each controller node should have a management connection to the Service Node. And with NVP 3.x, Service Node has an OpenFlow to two controller nodes, a master and its backup.	

### Hypervisor OVS

* Within your OpenStack Cloud, launch two instances of *Ubuntu-12.04-Server*.
* Associate a floating IP to your instance
* Run the following commands on the Ubuntu VM to install KVM on the node:

	apt-get update
	apt-get upgrade
	apt-get install kvm libvirt-bin 
	apt-get install dkms [required for OVS installation]

* You can now transfer and install OVS package to your nodes:

	scp nvp-ovs-x.x.x-buildxxxxx-ubuntu_amd64.tar.gz username@ubuntu-ip-address:
	tar –xvf nvp-ovs-x.x.x-buildxxxxx-ubuntu_amd64.tar.gz 
	dpkg –i  *.deb

* Connect to one of your hosts to validate IP routing:

	ssh nicira@10.10.1.3
	ifconfig eth1
	route -n
	ping -c 3 10.10.2.3

* Configure OVS switch on your hosts, do the following on each host to create the certificate to authenticate to NVP Control Cluster:

	sudo bash -login
	service openvswitch-switch status [to check service is running]
	cd /etc/openvswitch
	ovs-pki init --force
	ovs-pki req+sign ovsclient controller
	ovs-vsctl -- --bootstrap set-ssl /etc/openvswitch/ovsclient-privkey.pem /etc/openvswitch/ovsclient-cert.pem /etc/openvswitch/controller-ca-cert.pem
	ovs-vsctl set-manager ssl:<controllerIP> [point OVS to control cluster]


### Logical Networks

* Create a logical network
* Create logical ports
* attach VMs to the logical ports
* setup the NVP Gateway


### Links

* http://docs.openstack.org/trunk/openstack-network/admin/content/nvp_plugin.html

![][vin-archi]

VIN Components:

* Server in Infrastructure Navigator VM, transfers the data to the DB component
* DB in Infrastructure Navigator VM, stores data received from Server component.
* Infrastructure Navigator Plug-In in the vSphere Web Client, provides a GUI to view and analyse dependencies
* Inventory Service, a vSphere service in which VIN exports its data

You'll find a lot more details in the [VIN admin guide](http://www.vmware.com/pdf/vcenter-infrastructure-navigator-20-installation-administration-guide.pdf).

### Product Packaging

*VMware Infrastructure Navigator* is included with *vCops Advanced* and *Enterprise* version of the suite.

### Prerequisites and Architecture

If you don't have vCops installed in your environment, you have to start [here](/) first, come back here after.

Before starting your installation, it's a good idea to read the [release notes](http://www.vmware.com/support/adm/doc/vcenter-infrastructure-navigator-20-release-notes.html).  

VIN 2.0 is distributed as an *Open Virtualization Format (OVF)*, if you've done the vCops deployment you should already have it, if you need it you can access it [here](http://tryvmware.com):

|The following Ports are used by vCops||
|:-|:-|
|From your PC to VIN|
|5480|for appliance web console|
|5489|If VMware Update Manager (VUM) is installed|
|22|SSH access to VIN VM|
|vCenter to VIN|
|2868|For plugin download while registering it|
|6969|For connectivity with vSphere Web Client to VIN|
|VIN to vCenter|
|443|To access vSphere service API|
|80|To access vSphere Web service API|
|10109|To access vSphere Inventory Service|
|VIN to target hosts and VMs|
|443|Fox VIX protocol on target hosts to perform discovery|
|902|For VIX protocol on target hosts to perform discovery|

#### Required Components/Informations:

* VIN license key
* vSphere Web Client 5.1
* vCenter version 5.0 (registered with vSphere Web Client 5.1) or vCenter 5.1+
* ESX 3.5 P25, 4.0.0 U3, 4.1.0 P03, ESX 5.x+
* Predefined static IP/Netmask on desired Port Group
* Default Gateway, DNS.
* vCenter IP address and administrator credentials
* IE 7,8,9, Chrome 14, Firefox 3.6, Adobe Flash 11.1.0+

#### Registering vCenter with Web Sphere Client

If you have a vCenter version 5, you need to register it with a vSphere Web Client 5.1.
Login to your vSphere 5.1 appliance and type the following command

	/usr/lib/vmware-vsphere-client/scripts/admin-cmd.sh register https://<WebClient_IPorHostName>:<WebClient_HttpsPort>/vsphere-client <VC_IP> <VC_Admin-User> <VC_Admin-Passwd>

Example

	/usr/lib/vmware-vsphere-client/scripts/admin-cmd.sh register https://webclient.mydomain.com:9443/vsphere-client vc.mydomain.com administrator 'CENSORED'

If you use a windows vSphere 5.1 installation you can login to the server and access the following administrative tool instead :

	https://127.0.0.1:9443/admin-app

Then click on Register vCenter Server.

#### Virtual Machine resources requirements:

|:-|:-|
|CPU|2 vCPU|
|Memory|4 GB|
|Disk size|20 GB|
|Network|2 Gbps|

### Installation

If you have multiple vCenter, you need to deploy a separate instance of VIN for each vCenter.

1. Deploy OVF Template to target vCenter
2. Follow the wizard, select thin provisionned disk.
3. Select Power on after deployment and click Finish.
4. Log out from vSphere Web Client and log in again.
5. Enter VIN license.


#### Deploy vCops vApp

1. Connect to your vCenter
2. Enable DRS in the cluster
3. Verify that the VM network that you connect this vApp to has an IP Pool and select that network during the .ova deployment
4. File > Deploy OVF Template
5. Select Disk format, Thick provisioned eager-zeroed provides a 10% performance improvement (recommended)
6. Select Fixed or DHCP IP allocation
7. Finish
8. Power on the vCops Manager vApp
9. Check vCops VM IP address on Summary tab

#### Set ESX Host Time

1. Select host in the inventory
2. Configuration > Time Configuration > Properties to adjust time and click OK

#### Connect vCops to vCenter

1. http://<IP>/admin l:admin p:admin
2. update admin password (min 8 chars, one letter, one digit)
3. update root password (vmware by default)
4. type a name for the vCenter Server system (for reference only)
5. type IP or FQDN of vCenter
6. type registration credentials (require following access : Global: Licenses and Extension: Register extension, unregister extension, update extension.)
7. you can optionally enter collection credentials (usefull if you want to limit vCops to a subset of your datacenter)
8. click Save

#### Assign a license

1. vCenter > Home > Licensing
2. select Asset
3. right click your vCenter Operations edition and select Change License Key
4. select Assign a new license key to this solution
5. click Enter Key and enter it
6. click OK

#### SMTP/SNMP Configuration

1. connect to http://<ip>/admin with your new password
2. On the SMTP tab, select the Enable report email service check box.
3. Type the SMTP server address and port number.
4. Type the name and email address to use when sending alerts or reports.
5. If required select the Server requires an encrypted connection check box and select the encryption protocol.
6. If required select the Outgoing SMTP server requires authentication check box and type the credentials.
7. Select the Enable SNMP check box and type the destination host, port, and community information.
8. Click Update to apply your settings.

#### Verify installation

1. click vCenter > vCenter Operations Manager icon or access it from your browser http://<IP>/
2. ignore the certificate warnings that might appear.
3. Look at the dashboard and verify that the inventory objects you expect to see appear.

#### Installing a vCops Adapter

1. Obtain the PAK file for the adapter from VMware
2. connect to http://<IP>/admin
3. Update > Browse > locate your PAK file
4. click Update and click OK
5. Accept EULA and click OK
6. Login to https://<IP>/vcops-custom/
7. Select Admin > Support
8. On the Info tab, click the Describe icon in the Adapters pane.
9. Click Yes to start the describe process and click OK
10. Verify Build number which should match the one you've uploaded.

### Infrastructure Navigator Adapter

The *Infrastructure Navigator* adapter is an embedded adapter for vCops Advanced, it retrieves application-related information. You can customize it by modifying properties in the `vin_adapter.properties` file

|Property|Description|Default|
|:-|:-|:-|
|workerThreadCount|Number of threads for retrieving Infrastructure Navigator documents from the vCenter Inventory service. You can configure up to 100 threads.|10|
|docBuilderCount|Number of DocumentBuilder objects for parsing Infrastructure Navigator data documents. You can configure up to 100 objects.|10|
|syncInterval|Time interval, in minutes, that the adapter synchronizes its entire local cache with the vCenter Operations Manager Advanced server.|60|
|certCheckEnabled|Boolean value that determines whether the adapter checks the vCenter Operations Manager Advanced server certificate.|true in a vApp installation, false otherwise|
|vin20Enabled|Boolean value that determines whether to retrieve and process application-related information that is available only in Infrastructure Navigator 2.0.|true|

`/data/vcops/log/adapters/VinAdapter` Infrastructure Navigator adapter log files  
`/data/vcops/log` Collector log files on Analytics VM

To get more information you can set for a short interval DEBUG level logging.

### Upgrading

You can upgrade a *vCops* , 5.0.1, 5.0.2, 5.0.3 vAPP to *vCenter Operation Manager 5.6*x. It will require the following disk space :

|Resource|Min requirement|
|:-|:-|
|UI VM|*Disk 1*: 4GB & *Data Disks*: 250 GB|
|Analytics VM|*Disk 1*: 4 GB & *Data Disks*: 120 GB|

If you don't have enough available disk space, you can add virtual disks to the UI VM.  
Now to upgrade follow this procedure :

1. Save the .pak file of the latest vCenter Operations Manager Build ot your local storage
2. login to http://<IP>/admin
3. Update > Browse and select .pak file
4. click **Update**
5. Accept the EULA and click **OK**
6. Accept and confirm the update
7. login back to check the **Status** tab
8. login to vSphere Client or Custom UI to verify vCops interface is fine.

That's it we leave you with that. Enjoy *vCops* 5.6.

[vcops-dashboard]: /images/posts/vcops-dahboard.png
[vcops-details]: /images/posts/vcops-details.png width=750px