---
title: "VMware NSX 4.0 - lab installation"
created_at: 2013-12-31 19:48:00 +0100
kind: article
published: false
tags: ['howto', 'vmware', 'nsx']
---

In this article we will review all the steps required to deploy an OpenStack NSX PoC. We will embed this lab within an OpenStack cloud. In our environment each hypervisor VM is connected to both the management and data network using two distinct vNic network interfaces.

<!-- more -->

### OpenStack Networking

* Connect to your OpenStack Horizon dashboard and create the following networks
* be sure to leave Gateway unconfigured and leave DHCP unchecked.

|Network name|Subnet|
|CorpNAT|192.168.1.0/24|
|Management|10.0.0.0/24|
|transport-1|10.10.1.0/24|
|transport-2|10.10.2.0/24|

* Create a logical router to connect CorpNAT to the external world
* Create a logical router to connect both transport networks together

### NSX Controllers

The NVP Controllers Cluster is a cluster of x86 servers offering: massive scalability and full fault-tolerance. They manage the edge switching components within an NSX domain.

* Within your OpenStack Cloud, launch three instances of *NSX-Controller-4.x* (one for demo purpose is self-sufficient)
* Connect each of them to CorpNAT and Management Network
* Associate a floating IP to each instance
* Run the following commands on each of them, default login/password is admin/admin:

	ssh admin@controller-floatingIP
 	set hostname nsx-controller-<Instance#>
 	add network ntp-server 0.pool.ntp.org
 	set network interface breth0 ip config static <mgt-network-ip>.x 255.255.255.0
 	show network interface
 	set control-cluster management-address <controller-mgtIPaddress>
 	set control-cluster role api_provider listen-ip <controllerIP-mgtIPaddress>
 	set control-cluster role switch_manager listen-ip <controllerIP-mgtIPaddress>
 	join control-cluster <controllerIP-mgtIPaddress>
 	show control-cluster startup-nodes
 	show control-cluster status

If when joining you get the folllowing error :

	Could not join cluster, because a different node with this node's management address is already part of the cluster

You need to delete the old UUID from the Controller Cluster using the DELETE /ws.v1/control-cluster/node/<old_uuid> API method.

### NSX Manager

It's a web based interface that provides user-friendly method to interact with the NVP API. But it's not meant for day to day provisioning or automated configuration.

* Within your OpenStack Cloud, launch one instance of *NSX-Manager-4.x*.
* Connect it to both CorpNAT and Management networks
* Associate a floating IP to your instance
* Run the following commands, default login/password is admin/admin:

	ssh admin@manager-floatingIP
	set hostname nsx-manager
	show network dns-server
	add network dns-server 8.8.8.8 [not needed in 3.2 if DHCP configured, but needed if setup is empty]
	add network ntp-server 0.pool.ntp.org
	set network interface breth0 ip config static <mgt-network-ip>.x 255.255.255.0

* Connect to the Manager WebUI at http://manager-floatingIP with admin/admin credentials
* Click `Add Cluster`, Host = <controllerIP>, Username = admin, Password = admin
* To Create a `Transport Zone`, it's a collection of OVS devices that can communicate with each other, Click on Network Components > Transport Layer > Transport Zone

### NSX Gateway

The NVP Gateway will provide external access for our logical networks.
XXX

### Service Node

Service Node offloads task of packet replication from hypervisor CPU for L2 Broadcast, Unknown unicast and Multicast (BUM).

To use an alternative to provision a host, we will use PXE boot to provision our Service Node.

You first have to make sure your default security group allows UDP/69 traffic.

Before launching your instance, connect it 
to corpNAT, Management and Transport-1 Networks.

You can now launch a PXE instance and at the boot menu type what kind of instance you want to build based on the following pattern : 

	<node-type>-<releasecodename>-<buildnumber>

Where <node-type> could be controller, manager, service-node, gateway and <buildnumber> could be latest to get the latest working build. If you want the latest release you can simply type servicenode-latest

After a while you'll get your shiny new instance ready to rock.

* Associate a floating IP to it
* Run the following commands, default login/password is admin/admin:

	ssh admin@servicenode-floatingIP
 	set hostname nsx-servicenode
 	add network ntp-server 0.pool.ntp.org
 	show network interface
 	set network interface breth2 ip config static <mgt-network-ip>.x 255.255.255.0
 	set network interface breth0 ip config static <transport-network>.x 255.255.255.0
 	add network route 10.10.0.0 255.255.0.0 <network-data1>.1 [needed because communication with other hypervisors on other L2 should happen on the transport network]
 	show network routes
 	add switch manager <controllerIP>

* Connect to the Manager WebUI at http://manager-floatingIP with admin/admin credentials to create Service Node API Entity
* Click `Add` in the Service Node Row in Summary of Transport Components section
* Leave all values default exept
	Transport Node Type: Service Node
	Display Name: nsx-servicenode-1
* Get SSL certification from Service Node using:
	show switch certificate
* Paste SSL certificate in Security Certificate textbox
* Click `Add Connector`, to add a Transport Connector
	Transport Type: STT
	Transport Zone UUID: only one available here
	IP Address: <network-data1>.5
* Click `Save & View`, to register Service Node
* Check Service Node Status from Manager GUI, it should display Management Connection up,  OpenFlow Connection up and admin status enabled.
* Check Service Node Status from SSH
	show switch managers

Each controller node should have a management connection to the Service Node. And with NVP 3.x, Service Node has an OpenFlow to two controller nodes, a master and its backup.	

### Hypervisor OVS

* Within your OpenStack Cloud, launch two instances of *Ubuntu-12.04-Server*. (default login: nicira, password: nicira)
* Associate a floating IP to your instance
* Run the following commands on the Ubuntu VM to install KVM on the node:

	apt-get update
	apt-get upgrade
	apt-get install kvm libvirt-bin 
	apt-get install dkms [required for OVS installation]

* You can now transfer and install OVS package to your nodes:

	scp nvp-ovs-x.x.x-buildxxxxx-ubuntu_amd64.tar.gz username@ubuntu-ip-address:
	tar –xvf nvp-ovs-x.x.x-buildxxxxx-ubuntu_amd64.tar.gz 
	dpkg –i  *.deb

* Connect to one of your hosts to validate IP routing:

	ssh nicira@10.10.1.3
	ifconfig eth1
	route -n
	ping -c 3 10.10.2.3

* Configure OVS switch on your hosts, do the following on each host to create the certificate to authenticate to NVP Control Cluster:

	sudo bash -login
	service openvswitch-switch status [to check service is running]
	cd /etc/openvswitch
	ovs-pki init --force
	ovs-pki req+sign ovsclient controller
	ovs-vsctl -- --bootstrap set-ssl /etc/openvswitch/ovsclient-privkey.pem /etc/openvswitch/ovsclient-cert.pem /etc/openvswitch/controller-ca-cert.pem
	ovs-vsctl set-manager ssl:<controllerIP> [point OVS to control cluster]


### Logical Networks

* Create a logical network
* Create logical ports
* attach VMs to the logical ports
* setup the NVP Gateway

### Links

* http://docs.openstack.org/trunk/openstack-network/admin/content/nvp_plugin.html


[vcops-dashboard]: /images/posts/vcops-dahboard.png
[vcops-details]: /images/posts/vcops-details.png width=750px